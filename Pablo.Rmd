---
title: "JMJPFU-Prablo"
output: html_notebook
---

# JMJPFU
## 15-Feb-2017

This notebook is to demonstrate the Bayesian exercises as listed in Lecture PDF


### Example 1 : Three coin simulation

This is a simulation example to study the simulation test of 3 coin flipping

```{r}

# Defining a uniform prior for the three coins
coinPrior <- rep(1/3,3)

# Defining the theta value - This indicates the probability of heads of each coin

theta <- c(0.25,0.5,0.75)

# Defining the simulations

sim <- 100000

y <- rep(0,3*sim) # Defining a large vector of 0's

dim(y) <- c(sim,3)

# Defining a monte Carlo simulation

for (i in 1:sim){
  # In the first step we are sampling one coin from the list of 3 with equal probability of 1/3
  ind <- sample(c(1,2,3),size=1,prob=coinPrior,replace = TRUE)
  # In this step we are taking one observation, with size 1 with a probability equal to the index which we earlier sampled
  y.new <- rbinom(1,1,prob=theta[ind])
  
  y[i,ind] <- y.new
  
}

# Averaging each class and normalizing

prior <- apply(y,2,mean)/sum(apply(y,2,mean))

```

What we are doing above is taking a sample from a probability distribution and then taking a binomial distribution according to the theta we sampled . The exercise is simulated for a large number of times. Finally what we get is the posterior probability of heads for each of the coins

The estimated standard error of the monte carlo simulation is as follows

```{r}
thetaPost <- apply(y,2,mean)/sum(apply(y,2,mean))

V <- thetaPost * (1-thetaPost)/sim

sqrt(V)
```

### Calculating the Posterior Predictive Distribution

To find out the probability that the next flip will be head we have to multiply the posterior probability with the prior probability and sum over all the values of theta

= (0.25 * 0.167 + 0.50 * 0.33 + 0.75 * 0.499)

```{r}
sum(theta * apply(y,2,mean)/sum(apply(y,2,mean)))

```

# JMJPFU
### 16-Feb-2017


Sequential learning : If we obtain data y1 and find the posterior with data y1 and obtain new data y2 we will find the new posterior with prior as the posterior for data y1

"Todays posterior is tomorrow prior"

### Example 2 : New data y2

Let us take an example that the new data y2 = 0 ie it comes up tails. How is our beliefs revised now

Below is a function to find out the likelihood for a binomial case

```{r}
likeMaker <- function(theta,y){
  
  like = (theta^y) * ((1-theta)^(1-y))
  
  return(like)
  
}


```


```{r}
likeMaker(0.25,0)
```

To run the above example let us make a small function 

```{r}

postMaker <- function(prior,theta,data){
  # prior is a vector of priors
  # Theta is another vector of bias of the coin
  # Data is the y value you want to observe
  
  # First creating a data frame for loading the values in the table
  postDf <- data.frame(matrix(nrow=length(prior),ncol=5))
  # Creting the names
  
  names(postDf) <- c("Theta","Prior","Likelihood","bayesNum","Posterior")
  
  # Now loading values in the dataframe
  
  for(i in 1:nrow(postDf)){ # Iterating over the rows of the DF
    
    
      
       postDf[i,1] <- theta[i] # First column for the theta value
       postDf[i,2] <- prior[i] # Second column for prior
       postDf[i,3] <- likeMaker(theta[i],data) # likelihood stored in the third column
       postDf[i,4] <-  postDf[i,2] *  postDf[i,3] # Calcualting the numerator of Bayes equation
    
    
    
  }# End of row iteration
  
  normalizer <- sum(postDf[,4]) # This is the sum of the numerator of bayes equation
  
  postDf[,5] <- postDf[,4]/normalizer # This si the posterior
  
  return(postDf)
  
  
} # End of the function

```


Let us try out the above function

```{r}
theta <- theta # Line 23 above
prior <- prior # line 47 above

results <- postMaker(prior,theta,1) # Creting the data frame as above

prior <- results$Posterior

results1 <- postMaker(prior,theta,1) # Updated results after the above

```

Thank you Lord

Now once the above function is made, we will experiment further with more examples.
